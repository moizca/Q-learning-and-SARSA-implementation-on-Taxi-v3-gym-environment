{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RL_algo_Q_learn_SARSA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEZQ/ewgVU8lGKdWngzcLu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moizca/Q-learning-and-SARSA-implementation-on-Taxi-v3-gym-environment/blob/main/RL_algo_Q_learn_SARSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlRRRCy57uHE"
      },
      "source": [
        "\n",
        "```\n",
        "[# This code is made with the help of the commit on github with SHA1-hash 53bd33e8d78311ebdd46450d9956a5f3af30f9a5 and github link](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python/commits/master/Chapter04/SARSA%20Q_learning%20Taxi-v2.py)\n",
        "```\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUsoCRc-9GhE"
      },
      "source": [
        "##Please give me feedback on my first reinforcement code, say tuned as new amazing Reinforcement learning systems will be made soon!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCOLcey1mz2q",
        "outputId": "0089ffb2-72b6-479d-a6b5-cf8d4c4479e8"
      },
      "source": [
        "# Importing libraries.\n",
        "import numpy as np \n",
        "import gym\n",
        "\n",
        "# Epsilon Greedy action selection function with respect to a given state s.\n",
        "def eps_greedy(Q, s, eps=0.1):\n",
        "\n",
        "    if np.random.uniform(0,1) < eps:\n",
        "        return np.random.randint(Q.shape[1])\n",
        "    else:\n",
        "        return greedy(Q, s)\n",
        "\n",
        "# Greedy action selection function with  respect to a given state s, Epsilon Greedy action selection function also uses it.\n",
        "def greedy(Q, s):\n",
        "\n",
        "    return np.argmax(Q[s])\n",
        "\n",
        "# This function runs test epsidos to test the agent, and measure the performance of it by reporting the reward accumulated by the agent during the whole testing run.\n",
        "def run_episodes(env, Q, num_episodes=100, to_print=False):\n",
        "\n",
        "    tot_rew = []                           # An array which stores the cumulative reward of the agent in a testing epsisode.\n",
        "    state = env.reset()                    # Initialize the episode by sampling the initial state from the starting destribution over states.\n",
        "\n",
        "    for _ in range(num_episodes):          # The loop which runs the testing episodes\n",
        "        done = False\n",
        "        game_rew = 0\n",
        "\n",
        "        while not done:                    # This loop will run until the state become terminal state.\n",
        "            action = greedy(Q, state)      # Calls the greedy function as it follows the greedy policy, based on the table 'Q'.\n",
        "            next_state, rew, done, _ = env.step(action)          # At this point agent is giving action to the environment and the environment in return will give next state, reward and gives us some additional information like whether the next state is terminal or not.\n",
        "\n",
        "            state = next_state\n",
        "            game_rew += rew                # Rewards gets accumulated with in an episode in 'game_rew' variable.\n",
        "            if done:                       # When the state becomes terminal then the episode terminates, a new one may begain.\n",
        "                state = env.reset()\n",
        "                tot_rew.append(game_rew)   # Total accumulated reward of the terminated episode will be accumulated in 'tot_rew' array.\n",
        "\n",
        "    if to_print:\n",
        "        print('Mean score: %.3f of %i games!'%(np.mean(tot_rew), num_episodes))\n",
        "\n",
        "    return np.mean(tot_rew)\n",
        "\n",
        "def Q_learning(env, lr=0.01, num_episodes=10000, eps=0.3, gamma=0.95, eps_decay=0.00005):     # Lookup table based Q-learning will occur in this function. Q-learning is an off policy learning algorithm. Here we follow epsilon greedy policy as our behaviour policy but learning the Q table of the greedy one.\n",
        "    nA = env.action_space.n                                                    # Size of action space as an integer\n",
        "    nS = env.observation_space.n                                                # Size of state space as an integer\n",
        "\n",
        "    Q = np.zeros((nS, nA))                                                      # Initialize the Q table with zero values.\n",
        "    games_reward = []                                                           # This empty list will store the cumulative rewards during training.\n",
        "    test_rewards = []                                                           # This empty list will store cumulative rewards during testing.\n",
        "\n",
        "    for ep in range(num_episodes):                                              # This loop will run the episodes\n",
        "        state = env.reset()                                                     # Sample the initial state from the destribution over states.\n",
        "        done = False\n",
        "        tot_rew = 0\n",
        "\n",
        "        if eps > 0.01:                                                          # We have decay rate of exploration rate, by which we want to decay our exploration rate after each episode if exploration rate is greater then 0.01\n",
        "            eps -= eps_decay\n",
        "\n",
        "        while not done:                                                         # Here the training episode will run till the terminal episode.\n",
        "\n",
        "            action = eps_greedy(Q, state, eps)                                  # We select our action based on the state and the epsilon greedy policy which is our behaviour policy.\n",
        "\n",
        "            next_state, rew, done, _ = env.step(action)                         # Action is given to the environment and next state, reward and other information will be given by the environment.\n",
        "\n",
        "            Q[state][action] = Q[state][action] + lr*(rew + gamma*np.max(Q[next_state]) - Q[state][action])       # Q value is updated in the Q-table with the learning rate of 'lr = 0.01'.\n",
        "                                                                                # Here, the update is made by bootstrapping from the Q-value of the action which has the maximum value, over the next state.\n",
        "            state = next_state\n",
        "            tot_rew += rew                                                      # Training reward gets accumulated in the 'tot_rew variable'.\n",
        "            if done:\n",
        "                games_reward.append(tot_rew)                                    # At the end of the episode the cumulative reward of the episode will be stored in 'games_reward' variable.\n",
        "\n",
        "        if (ep % 300) == 0:                                                     # Testing episodes are initialized at every 300th of training episode.\n",
        "            test_rew = run_episodes(env, Q, 1000)\n",
        "            print(\"Episode:{:5d}  Eps:{:2.4f}  Rew:{:2.4f}\".format(ep, eps, test_rew))\n",
        "            test_rewards.append(test_rew)\n",
        "            \n",
        "    return Q                                                                    # Learned Q-table will be returned.\n",
        "\n",
        "\n",
        "def SARSA(env, lr=0.01, num_episodes=10000, eps=0.3, gamma=0.95, eps_decay=0.00005):     # Lookup table based SARSA (State, Action, Reward, State, Action) will occur in this function. SARSA is an on-policy learning algorithm. Here, we follow epsilon greedy policy as our behaviour policy which is also the policy for which Q table is learned. It is like Temporal Difference (TD) method but for action values, not for state values\n",
        "    nA = env.action_space.n                                                     # Size of action space as an integer\n",
        "    nS = env.observation_space.n                                                # Size of state space as an integer\n",
        "\n",
        "\n",
        "    Q = np.zeros((nS, nA))                                                      # Initialize the Q table with zero values.\n",
        "    games_reward = []                                                           # This empty list will store the cumulative rewards during training.\n",
        "    test_rewards = []                                                           # This empty list will store cumulative rewards during testing.\n",
        "\n",
        "    for ep in range(num_episodes):                                              # This loop will run the episodes\n",
        "        state = env.reset()                                                     # Sample the initial state from the destribution over states.\n",
        "        done = False\n",
        "        tot_rew = 0\n",
        "\n",
        "        if eps > 0.01:                                                          # We have decay rate of exploration rate, by which we want to decay our exploration rate after each episode if exploration rate is greater then 0.01\n",
        "            eps -= eps_decay\n",
        "\n",
        "\n",
        "        action = eps_greedy(Q, state, eps) \n",
        "\n",
        "        while not done:                                                         # Here the training episode will run till the terminal episode.\n",
        "            next_state, rew, done, _ = env.step(action) \n",
        "\n",
        "            next_action = eps_greedy(Q, next_state, eps)                        # We select our action based on the state and the epsilon greedy policy which is our behaviour policy as well as target policy. We are doing value iteration instead of policy iteration.\n",
        "\n",
        "            Q[state][action] = Q[state][action] + lr*(rew + gamma*Q[next_state][next_action] - Q[state][action])       # Q value is updated in the Q-table with the learning rate of 'lr = 0.01'.\n",
        "                                                                                # Here, the update is made by bootstrapping from the Q-value of the action which over the next state which is choosen by our behaviour policy.\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "            tot_rew += rew                                                      # Training reward gets accumulated in the 'tot_rew variable'.\n",
        "            if done:\n",
        "                games_reward.append(tot_rew)                                    # At the end of the episode the cumulative reward of the episode will be stored in 'games_reward' variable.\n",
        "\n",
        "        if (ep % 300) == 0:                                                     # Testing episodes are initialized at every 300th of training episode.\n",
        "            test_rew = run_episodes(env, Q, 1000)\n",
        "            print(\"Episode:{:5d}  Eps:{:2.4f}  Rew:{:2.4f}\".format(ep, eps, test_rew))\n",
        "            test_rewards.append(test_rew)\n",
        "\n",
        "    return Q                                                                    # Learned Q-table will be returned.\n",
        "\n",
        "\n",
        "if __name__ == '__main__':                                                      # Code under this if statement will only run in this file. If this python file is imported in some other file and then used, this part of code cannot be run.\n",
        "    env = gym.make('Taxi-v3')                                                   # The environment 'Taxi-v3' from 'open-ai gym' is created.\n",
        "    \n",
        "    Q_qlearning = Q_learning(env, lr=.1, num_episodes=5000, eps=0.4, gamma=0.95, eps_decay=0.001)       # Q_learning function is called to initialize the Q-learning.\n",
        "\n",
        "    Q_sarsa = SARSA(env, lr=.1, num_episodes=5000, eps=0.4, gamma=0.95, eps_decay=0.001)                # SARSA function is called to initialize the learning based on SARSA algorithm."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:    0  Eps:0.3990  Rew:-235.6850\n",
            "Episode:  300  Eps:0.0990  Rew:-204.4050\n",
            "Episode:  600  Eps:0.0100  Rew:-178.6070\n",
            "Episode:  900  Eps:0.0100  Rew:-126.8770\n",
            "Episode: 1200  Eps:0.0100  Rew:-153.8570\n",
            "Episode: 1500  Eps:0.0100  Rew:-76.5620\n",
            "Episode: 1800  Eps:0.0100  Rew:-35.6510\n",
            "Episode: 2100  Eps:0.0100  Rew:-41.7790\n",
            "Episode: 2400  Eps:0.0100  Rew:-32.0440\n",
            "Episode: 2700  Eps:0.0100  Rew:-5.4690\n",
            "Episode: 3000  Eps:0.0100  Rew:-12.9770\n",
            "Episode: 3300  Eps:0.0100  Rew:5.6800\n",
            "Episode: 3600  Eps:0.0100  Rew:6.3220\n",
            "Episode: 3900  Eps:0.0100  Rew:7.3270\n",
            "Episode: 4200  Eps:0.0100  Rew:7.8910\n",
            "Episode: 4500  Eps:0.0100  Rew:7.9700\n",
            "Episode: 4800  Eps:0.0100  Rew:7.9920\n",
            "Episode:    0  Eps:0.3990  Rew:-200.0000\n",
            "Episode:  300  Eps:0.0990  Rew:-210.4260\n",
            "Episode:  600  Eps:0.0100  Rew:-212.5980\n",
            "Episode:  900  Eps:0.0100  Rew:-155.3150\n",
            "Episode: 1200  Eps:0.0100  Rew:-198.9420\n",
            "Episode: 1500  Eps:0.0100  Rew:-63.7550\n",
            "Episode: 1800  Eps:0.0100  Rew:-30.9500\n",
            "Episode: 2100  Eps:0.0100  Rew:-23.9860\n",
            "Episode: 2400  Eps:0.0100  Rew:-22.0760\n",
            "Episode: 2700  Eps:0.0100  Rew:-0.8250\n",
            "Episode: 3000  Eps:0.0100  Rew:2.5800\n",
            "Episode: 3300  Eps:0.0100  Rew:5.1490\n",
            "Episode: 3600  Eps:0.0100  Rew:7.2700\n",
            "Episode: 3900  Eps:0.0100  Rew:7.9530\n",
            "Episode: 4200  Eps:0.0100  Rew:7.8880\n",
            "Episode: 4500  Eps:0.0100  Rew:7.7560\n",
            "Episode: 4800  Eps:0.0100  Rew:7.8250\n"
          ]
        }
      ]
    }
  ]
}